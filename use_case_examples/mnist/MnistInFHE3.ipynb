{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Based on this tutorial https://www.zama.ai/post/quantization-of-neural-networks-for-fully-homomorphic-encryption*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a 3-layer MNIST model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from threading import Thread\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from concrete.ml.torch import NumpyModule\n",
    "from concrete.ml.quantization import PostTrainingAffineQuantization\n",
    "from concrete.ml.quantization import QuantizedArray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.1307,), (0.3081,))\n",
    "        ])\n",
    "\n",
    "train_data = torchvision.datasets.MNIST('./data', train=True, download=False,\n",
    "                    transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST('./data', train=False,\n",
    "                    transform=transform)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, 10000, shuffle=False)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(in_features=28*28, out_features=128, bias=False)\n",
    "        self.sigmoid1 = nn.Sigmoid()\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64, bias=False)\n",
    "        self.sigmoid2 = nn.Sigmoid()\n",
    "        self.fc3 = nn.Linear(in_features=64, out_features=10, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.sigmoid1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "        \n",
    "net = Model()\n",
    "\n",
    "def test(epoch):\n",
    "    net.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(test_loader):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.reshape(inputs.shape[0], inputs.shape[1]* inputs.shape[2]*inputs.shape[3])\n",
    "            outputs = net(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "        print(f\"Epoch {epoch}, Test accuracy: {np.round(100.*correct/total, 2)}, Correct: {correct}, Total: {total}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001,momentum=0.9, weight_decay=5e-4)\n",
    "net = net.to(device)\n",
    "net.train()\n",
    "\n",
    "def train(epochs):\n",
    "    for epoch in range(epochs):\n",
    "        for i, (inputs, targets) in enumerate(train_loader, 0):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.reshape(inputs.shape[0], inputs.shape[1]*inputs.shape[2]*inputs.shape[3])\n",
    "            optimizer.zero_grad()\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        test(epoch)\n",
    "\n",
    "    print('Finished Training. Model saved to \"mnist_97p.pt\"')\n",
    "    torch.save(net.state_dict(), \"mnist_97p.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_thread = Thread(target=train, name=\"train mnist\", args=(50, ))\n",
    "train_thread.is_alive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Test accuracy: 86.98, Correct: 8698, Total: 10000\n"
     ]
    }
   ],
   "source": [
    "train_thread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Test accuracy: 96.38, Correct: 9638, Total: 10000\n",
      "Epoch 19, Test accuracy: 96.56, Correct: 9656, Total: 10000\n",
      "Epoch 20, Test accuracy: 96.53, Correct: 9653, Total: 10000\n",
      "Epoch 21, Test accuracy: 96.59, Correct: 9659, Total: 10000\n",
      "Epoch 22, Test accuracy: 96.66, Correct: 9666, Total: 10000\n",
      "Epoch 23, Test accuracy: 96.79, Correct: 9679, Total: 10000\n",
      "Epoch 24, Test accuracy: 96.78, Correct: 9678, Total: 10000\n",
      "Epoch 25, Test accuracy: 96.79, Correct: 9679, Total: 10000\n",
      "Epoch 26, Test accuracy: 96.8, Correct: 9680, Total: 10000\n",
      "Epoch 27, Test accuracy: 96.92, Correct: 9692, Total: 10000\n",
      "Epoch 28, Test accuracy: 97.01, Correct: 9701, Total: 10000\n",
      "Epoch 29, Test accuracy: 96.95, Correct: 9695, Total: 10000\n",
      "Epoch 30, Test accuracy: 97.03, Correct: 9703, Total: 10000\n",
      "Epoch 31, Test accuracy: 97.1, Correct: 9710, Total: 10000\n",
      "Epoch 32, Test accuracy: 97.17, Correct: 9717, Total: 10000\n",
      "Epoch 33, Test accuracy: 97.14, Correct: 9714, Total: 10000\n",
      "Epoch 34, Test accuracy: 97.21, Correct: 9721, Total: 10000\n",
      "Epoch 35, Test accuracy: 97.21, Correct: 9721, Total: 10000\n",
      "Epoch 36, Test accuracy: 97.23, Correct: 9723, Total: 10000\n",
      "Epoch 37, Test accuracy: 97.24, Correct: 9724, Total: 10000\n",
      "Epoch 38, Test accuracy: 97.35, Correct: 9735, Total: 10000\n",
      "Epoch 39, Test accuracy: 97.33, Correct: 9733, Total: 10000\n",
      "Epoch 40, Test accuracy: 97.3, Correct: 9730, Total: 10000\n",
      "Epoch 41, Test accuracy: 97.32, Correct: 9732, Total: 10000\n",
      "Epoch 42, Test accuracy: 97.33, Correct: 9733, Total: 10000\n",
      "Epoch 43, Test accuracy: 97.33, Correct: 9733, Total: 10000\n",
      "Epoch 44, Test accuracy: 97.36, Correct: 9736, Total: 10000\n"
     ]
    }
   ],
   "source": [
    "train_thread.is_alive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Test accuracy: 97.44, Correct: 9744, Total: 10000\n",
      "Epoch 47, Test accuracy: 97.41, Correct: 9741, Total: 10000\n",
      "Epoch 48, Test accuracy: 97.48, Correct: 9748, Total: 10000\n",
      "Epoch 49, Test accuracy: 97.45, Correct: 9745, Total: 10000\n",
      "Finished Training. Model saved to \"mnist_97p.pt\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
       "  (sigmoid1): Sigmoid()\n",
       "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (sigmoid2): Sigmoid()\n",
       "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_thread.join()\n",
    "PATH = \"./mnist_97p.pt\"\n",
    "torch_fc_model = Model()\n",
    "torch_fc_model.load_state_dict(torch.load(PATH, map_location=torch.device('cpu')))\n",
    "torch_fc_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============= Diagnostic Run torch.onnx.export version 2.0.1+cu117 =============\n",
      "verbose: False, log level: Level.ERROR\n",
      "======================= 0 NONE 0 NOTE 0 WARNING 0 ERROR ========================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9745"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create some dummy input for the tracing.\n",
    "dummy_data = torch.randn(1, 28*28)\n",
    "\n",
    "# Carry torch model to numpy (required in by concrete)\n",
    "numpy_fc_model = NumpyModule(torch_fc_model, dummy_data)\n",
    "\n",
    "# Create random inputs of (n_examples, n_features)\n",
    "mnist_test_data, mnist_test_target = next(iter(test_loader))\n",
    "mnist_test_data = mnist_test_data.detach().numpy()\n",
    "mnist_test_target = mnist_test_target.detach().numpy()\n",
    "mnist_test_data = mnist_test_data.reshape(mnist_test_data.shape[0], mnist_test_data.shape[1]* mnist_test_data.shape[2]*mnist_test_data.shape[3])\n",
    "\n",
    "# Check that both model give same output\n",
    "# Accuracy pytorch\n",
    "(torch_fc_model(torch.from_numpy(mnist_test_data)).detach().numpy().argmax(1) == mnist_test_target).mean()\n",
    "# Output: 0.9733\n",
    "\n",
    "# Accuracy numpy\n",
    "(numpy_fc_model(mnist_test_data).argmax(1) == mnist_test_target).mean()\n",
    "# Output: 0.9733"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantized inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bits = 6  # 64\n",
    "is_signed = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantizing our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_quant = PostTrainingAffineQuantization(n_bits = n_bits, \n",
    "                                          numpy_model = numpy_fc_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<concrete.ml.quantization.post_training.PostTrainingAffineQuantization at 0x7f1fc19819d0>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calibrate layers and activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_module = pt_quant.quantize_module(mnist_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quantize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_mnist_test_data = QuantizedArray(n_bits = n_bits, \n",
    "                                   values=mnist_test_data, \n",
    "                                   is_signed=is_signed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare dequantized input value vs real input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "real value = [0.64495873 1.9305104  1.5995764  1.4977505  0.33948106 0.03400347\n",
      " 2.401455   2.8087585  2.8087585  2.8087585  2.8087585  2.6432915\n",
      " 2.0959773  2.0959773  2.0959773  2.0959773 ] \n",
      "\n",
      "dequantized value = [0.64495873 1.9305104  1.5995764  1.4977505  0.33948106 0.03400347\n",
      " 2.401455   2.8087585  2.8087585  2.8087585  2.8087585  2.6432915\n",
      " 2.0959773  2.0959773  2.0959773  2.0959773 ] \n",
      "\n",
      "quantized value = [21 45 39 37 15  9 55 63 63 63 63 59 49 49 49 49]\n"
     ]
    }
   ],
   "source": [
    "arg_diff_values = (mnist_test_data != -0.42421296)\n",
    "# Real input\n",
    "print(f\"real value = {mnist_test_data[arg_diff_values][:16]} \\n\")\n",
    "print(f\"dequantized value = {mnist_test_data[arg_diff_values][:16]} \\n\")\n",
    "print(f\"quantized value = {q_mnist_test_data.qvalues[arg_diff_values][:16]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the quantized weights for the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2,  0,  0,  0,  0, -1,  1,  0,  0,  2,  0,  1,  1,  1,  2,  0])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(quant_module.quant_layers_dict.values()))[1].constant_inputs[1].qvalues[0][:16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all input values are integers with 2**6 (64) values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n",
       "       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(q_mnist_test_data.qvalues[arg_diff_values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4507"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Accuracy Quantized Numpy (6 bits)\n",
    "(quant_module.quantized_forward(q_mnist_test_data.qvalues).argmax(1) == mnist_test_target).mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "concrete-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
